# Comparison of results from multi-layer perceptron models #



Multi-Layer Perceptron: 
	Accuracy:  
	Precision: 
	Recall:    


/////////////////////////////////////////////////////////////////////////////////////////////
1 Hidden Layer, 10 units
Conclusion: 
Useless, non-functional like linear regression
/////////////////////////////////////////////////////////////////////////////////////////////

julia> testModel(mlp_1layer10)
              precision    recall  f1-score   support

           0       0.87      0.98      0.92    113188
           1       0.00      0.00      0.00     17230

    accuracy                           0.85    130418
   macro avg       0.43      0.49      0.46    130418
weighted avg       0.75      0.85      0.80    130418
              precision    recall  f1-score   support

           0       0.86      0.98      0.92     55700
           1       0.00      0.00      0.00      8537

    accuracy                           0.85     64237
   macro avg       0.43      0.49      0.46     64237
weighted avg       0.75      0.85      0.80     64237

/////////////////////////////////////////////////////////////////////////////////////////////
1 Hidden Layer, 25 units
Conclusion: 
Also non-functional.
/////////////////////////////////////////////////////////////////////////////////////////////

              precision    recall  f1-score   support

           0       0.87      0.98      0.92    113188
           1       0.00      0.00      0.00     17230

    accuracy                           0.85    130418
   macro avg       0.43      0.49      0.46    130418
weighted avg       0.75      0.85      0.80    130418
              precision    recall  f1-score   support

           0       0.86      0.98      0.92     55700
           1       0.00      0.00      0.00      8537

    accuracy                           0.85     64237
   macro avg       0.43      0.49      0.46     64237
weighted avg       0.75      0.85      0.80     64237

/////////////////////////////////////////////////////////////////////////////////////////////
1 Hidden Layer, 50 units
Conclusion: 
as good as decision tree roughly
/////////////////////////////////////////////////////////////////////////////////////////////


julia> testModel(mlp_1layer)
C:\Users\Matei\.julia\conda\3\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  warnings.warn(
              precision    recall  f1-score   support

           0       0.97      0.96      0.97    113188
           1       0.77      0.81      0.79     17230

    accuracy                           0.94    130418
   macro avg       0.87      0.88      0.88    130418
weighted avg       0.94      0.94      0.94    130418
              precision    recall  f1-score   support

           0       0.97      0.96      0.97     55700
           1       0.77      0.81      0.79      8537

    accuracy                           0.94     64237
   macro avg       0.87      0.89      0.88     64237
weighted avg       0.94      0.94      0.94     64237


/////////////////////////////////////////////////////////////////////////////////////////////
1 Hidden Layer, 100 units
Conclusion:
Not meaningfully better than 50 units
/////////////////////////////////////////////////////////////////////////////////////////////

              precision    recall  f1-score   support

           0       0.97      0.96      0.97    113188
           1       0.75      0.83      0.79     17230

    accuracy                           0.94    130418
   macro avg       0.86      0.90      0.88    130418
weighted avg       0.95      0.94      0.94    130418
              precision    recall  f1-score   support

           0       0.97      0.96      0.97     55700
           1       0.76      0.83      0.79      8537

    accuracy                           0.94     64237
   macro avg       0.87      0.90      0.88     64237
weighted avg       0.95      0.94      0.94     64237